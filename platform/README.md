# Platform Server: The Agentic Orchestration API Layer

## Overview

**Open Source ‚Ä¢ Community-Driven ‚Ä¢ Apache 2.0 Licensed**

> **Built-in Agentic Tools. Built-in Orchestration. Built-in MCP Support. Any Model.**  
> No glue code. Use **any framework** that speaks Completions or Responses API<sup>1</sup>.

<sup>1</sup> OpenAI Agents SDK, Autogen, LangGraph, CrewAI, Agno Agents, LMOS ARC, and more.

**Platform Server** is an agentic orchestration API layer that provides drop-in replacement for `/responses`, `/chat/completions`, and `/embeddings` ‚Äî now with:

- üîß **Built-in agentic tool/function calling**
- ‚úÖ **Built-in Remote MCP server Integrations**
- üîç **Built-in Search + RAG**
- üß† **Built-in Agentic state + memory**
- üéØ **Built-in agent orchestration capabilities**

Works even if your model lacks native support ‚Äî like [OpenAI's Responses API](https://platform.openai.com/docs/api-reference/responses).

## üîß Key Engineering Wins

üß† **Built-In Agentic Tools, Server-Side**  
RAG, file/web search, memory, and remote MCP server integrations ‚Äî all built-in with zero glue code.

‚ö° **Fast, Flexible, and Fully Open**  
Supports any model, stateful responses, and tool/function calling ‚Äî lightweight, high-performance, and easy to self-host.

üéØ **Agentic Orchestration**  
Multi-agent workflows, collaboration, and agent-to-agent messaging capabilities built-in.

## üöÄ Getting Started

Get up and running in **2 steps** ‚Äî an agentic orchestration API with tool calling, RAG, memory, and remote MCP, powered by **your models**.

### üê≥ Run with Docker

```bash
docker run -p 6644:6644 masaicai/open-responses:latest
```

### Using with OpenAI SDK

```python
openai_client = OpenAI(base_url="http://localhost:6644/v1", api_key=os.getenv("XAI_API_KEY"))

response = openai_client.responses.create(
    model="xai@grok-4-0709",
    input="Write a poem on Masaic"
)
```

### Using with OpenAI Agent SDK

```python
client = AsyncOpenAI(base_url="http://localhost:6644/v1", api_key=os.getenv("XAI_API_KEY"))
agent = Agent(
    name="Assistant",
    instructions="You are a humorous poet who can write funny poems of 4 lines.",
    model=OpenAIResponsesModel(model="xai@grok-4-0709", openai_client=client)
)
```

### Using with cURL

```bash
curl --location 'http://localhost:6644/v1/responses' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer XAI_API_KEY' \
--data '{
    "model": "xai@grok-4-0709",
    "stream": false,
    "input": [
        {
            "role": "user",
            "content": "Write a poem on Masaic"
        }
    ]
}'
```

## Core Capabilities

| Feature | Description | Benefit |
|---------|-------------|---------|
| **Agentic Orchestration** | Multi-agent workflows and collaboration | Build complex agent systems with minimal code |
| **Automated Tracing** | Comprehensive request and response monitoring | Track performance and usage without additional code |
| **Integrated RAG** | Contextual information retrieval | Enhance responses with relevant external data automatically |
| **Pre-built Tool Integrations** | Web search, GitHub access, and more | Deploy advanced capabilities instantly |
| **Self-Hosted Architecture** | Full control of deployment infrastructure | Maintain complete data sovereignty |
| **OpenAI-Compatible Interface** | Drop-in replacement for existing OpenAI implementations | Minimal code changes for migration |

## API Reference

The API implements the following OpenAI-compatible endpoints:

### Responses API

| Endpoint | Description |
|----------|-------------|
| `POST /v1/responses` | Create a new model response | 
| `GET /v1/responses/{responseId}` | Retrieve a specific response | 
| `DELETE /v1/responses/{responseId}` | Delete a response | 
| `GET /v1/responses/{responseId}/input_items` | List input items for a response | 

### Completions API

| Endpoint | Description |
|----------|-------------|
| `POST /v1/chat/completions` | Create a chat completion (OpenAI compatible) |
| `GET /v1/chat/completions/{completionId}` | Retrieve a specific chat completion |
| `DELETE /v1/chat/completions/{completionId}` | Delete a chat completion |

### File Management

| Endpoint | Description |
|----------|-------------|
| `POST /v1/files` | Upload a file |
| `GET /v1/files` | List uploaded files |
| `GET /v1/files/{file_id}` | Retrieve file metadata |
| `DELETE /v1/files/{file_id}` | Delete a file |
| `GET /v1/files/{file_id}/content` | Retrieve file content |

### Vector Store Operations

| Endpoint | Description |
|----------|-------------|
| `POST /v1/vector_stores` | Create a vector store |
| `GET /v1/vector_stores` | List vector stores |
| `GET /v1/vector_stores/{vector_store_id}` | Retrieve a vector store |
| `POST /v1/vector_stores/{vector_store_id}` | Modify a vector store |
| `DELETE /v1/vector_stores/{vector_store_id}` | Delete a vector store |
| `POST /v1/vector_stores/{vector_store_id}/search` | Search a vector store |
| `POST /v1/vector_stores/{vector_store_id}/files` | Add a file to a vector store |
| `GET /v1/vector_stores/{vector_store_id}/files` | List files in a vector store |
| `GET /v1/vector_stores/{vector_store_id}/files/{file_id}` | Retrieve a vector store file |
| `GET /v1/vector_stores/{vector_store_id}/files/{file_id}/content` | Retrieve vector store file content |
| `POST /v1/vector_stores/{vector_store_id}/files/{file_id}` | Update vector store file attributes |
| `DELETE /v1/vector_stores/{vector_store_id}/files/{file_id}` | Delete a file from a vector store |

### Evaluations API

| Endpoint | Description |
|----------|-------------|
| `POST /v1/evals` | Create a new evaluation |
| `GET /v1/evals/{evalId}` | Retrieve a specific evaluation |
| `GET /v1/evals` | List evaluations with pagination and filtering |
| `DELETE /v1/evals/{evalId}` | Delete an evaluation |
| `POST /v1/evals/{evalId}` | Update an evaluation |
| `POST /v1/evals/{evalId}/runs` | Create a new evaluation run |
| `GET /v1/evals/{evalId}/runs/{runId}` | Retrieve a specific evaluation run |
| `GET /v1/evals/{evalId}/runs` | List evaluation runs for a specific evaluation |
| `DELETE /v1/evals/{evalId}/runs/{runId}` | Delete an evaluation run |

### Setting Up the OpenTelemetry Collector

1. To enable the OpenTelemetry collector integration, start the [service](open-responses-server) with:
   ```
   OTEL_SDK_DISABLED=false
   ```

2. The OpenTelemetry collector collects data from the service using OTLP (OpenTelemetry Protocol).

3. Configuration of the collector is done via its config file, typically located in the deployment environment.

4. All opentelemetry [sdk-environment-variables](https://opentelemetry.io/docs/specs/otel/configuration/sdk-environment-variables/) are supported.

### Platform Configuration For Local Development
```bash
OPEN_RESPONSES_STORE_VECTOR_SEARCH_QDRANT_API_KEY=
OPEN_RESPONSES_STORE_VECTOR_SEARCH_QDRANT_HOST=
OPEN_RESPONSES_STORE_VECTOR_SEARCH_QDRANT_USE_TLS=true/false
OPENAI_API_KEY=
SPRING_PROFILES_ACTIVE=platform
```

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

Before submitting a Pull Request, please ensure all regression tests pass by running:

```bash
./regression/regression_common.sh
./regression/regression_vector.sh
```
---

### Connect AgC MCP Server In Claude Desktop
1. Open Settings >> Developer >> Edit Config
2. This will open file claude_desktop_config.json
3. Add the following MCP server:
```json
{
  "mcpServers": {
    "AgC": {
      "command": "npx",
      "args": ["mcp-remote", "https://api.platform.dev.masaic.xyz/mcp", "--allow-http", "--header", "Authorization: Bearer ${MCP_API_KEY}"],
      "env": {
        "APP_MODE": "http",
        "MCP_API_KEY": "AgC_MCP_API_KEY"
      }
    }
  }
}
```
4. Save config and restart claude. You would see AgC in MCP servers.

<p align="center">
  Made with ‚ù§Ô∏è by the Masaic AI Team
</p>
